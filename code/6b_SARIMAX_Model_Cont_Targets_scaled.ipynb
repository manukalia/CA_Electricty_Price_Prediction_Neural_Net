{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">  \n",
    "\n",
    "<b> <font size='5'>  \n",
    "ELEC PRICE PREDICTION CAPSTONE:</font>  \n",
    "\n",
    "<font size='4'>SARIMAX Model for Continuous Targets (scaled exog var)</font> </b>\n",
    "\n",
    "<font size='3'>  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Manu Kalia Project Submission<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DSI-7-SF<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 15-May-2019<br>\n",
    "</font>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Load-Pickles:--Train/Test-Dataframes-&amp;--Scaled-Arrays\" data-toc-modified-id=\"Load-Pickles:--Train/Test-Dataframes-&amp;--Scaled-Arrays-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load Pickles:  Train/Test Dataframes &amp;  Scaled Arrays</a></span></li><li><span><a href=\"#Gridsearch-to-find-best-MSE-params-for-P,-D,-Q,-and-S\" data-toc-modified-id=\"Gridsearch-to-find-best-MSE-params-for-P,-D,-Q,-and-S-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Gridsearch to find best MSE params for <code>P</code>, <code>D</code>, <code>Q</code>, and <code>S</code></a></span></li><li><span><a href=\"#Instantiate-and-fit-the-models-with-best-params\" data-toc-modified-id=\"Instantiate-and-fit-the-models-with-best-params-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Instantiate and fit the models with best params</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import wget, os\n",
    "import time\n",
    "import glob\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "---\n",
    "\n",
    "## Load Pickles:  Train/Test Dataframes &  Scaled Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/train_sc_df.pkl', 'rb') as f:\n",
    "    train_sc_df = pickle.load(f)\n",
    "    \n",
    "with open('../data/processed/test_sc_df.pkl', 'rb') as f:\n",
    "    test_sc_df = pickle.load(f)\n",
    "        \n",
    "with open('../data/pre_processed_df.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "with open('../data/processed/train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "        \n",
    "with open('../data/processed/test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_col_for_dam = [train.columns[i] for i, col in enumerate(df.columns) if col != 'dam_price_per_mwh']\n",
    "exog_col_for_hasp = [train.columns[i] for i, col in enumerate(df.columns) if col != 'hasp_price_per_mwh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc_dam = train_sc_df[exog_col_for_dam]\n",
    "X_test_sc_dam  = test_sc_df[exog_col_for_dam]\n",
    "y_train_dam    = train['dam_price_per_mwh']\n",
    "y_test_dam     = test['dam_price_per_mwh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc_hasp = train_sc_df[exog_col_for_hasp]\n",
    "X_test_sc_hasp  = test_sc_df[exog_col_for_hasp]\n",
    "y_train_hasp    = train['hasp_price_per_mwh']\n",
    "y_test_hasp     = test['hasp_price_per_mwh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21800,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_dam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21800, 27)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sc_dam.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Gridsearch to find best MSE params for `P`, `D`, `Q`, and `S`\n",
    "\n",
    "(for `DAM` and `HASP` prices as endogenous variables,  \n",
    " use the ARIMA parameters for `p`, `d`, & `q` found in previous ARIMA gridsearch...  \n",
    " `DAM: 4, 0, 6`  and  `HASP: 6, 0 ,6`)\n",
    "\n",
    "> NOTE: Because of the amount of time each SARIMAX model takes to fit, there was simply not enough time to do these grid searches, although the code is correct and ready to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Matt Brem's grid search loop\n",
    "# Starting MSE and (P, D, Q).\n",
    "\n",
    "# mse = 99 * (10 ** 16)\n",
    "# final_P = 0\n",
    "# final_D = 0\n",
    "# final_Q = 0\n",
    "# final_S = 0\n",
    "\n",
    "# for P in range(3):\n",
    "#     for Q in range(3):\n",
    "#         for D in range(3):\n",
    "#             for S in range(0,24,8):\n",
    "#                 try:\n",
    "#                     # Instantiate SARIMA model.\n",
    "#                     sarima = SARIMAX(endog = y_train_dam,\n",
    "#                                      order = (4, 0, 6),              # (p, d, q)\n",
    "#                                      seasonal_order = (P, D, Q, S),  # (P, D, Q, S)\n",
    "#                                      exog = X_train_sc_dam) \n",
    "\n",
    "#                     # Fit SARIMA model.\n",
    "#                     model = sarima.fit()\n",
    "\n",
    "#                     # Generate predictions based on training set.\n",
    "#                     preds = model.predict()\n",
    "\n",
    "#                     # Evaluate predictions.\n",
    "#                     print(f'MSE for (4,0,6) x ({P},{D},{Q},{S}) ... {mean_squared_error(train[\"dam_price_per_mwh\"], preds)}')\n",
    "\n",
    "#                     # Save for final report.\n",
    "#                     if mse > mean_squared_error(train['dam_price_per_mwh'], preds):\n",
    "#                         mse = mean_squared_error(train['dam_price_per_mwh'], preds)\n",
    "#                         final_P = P\n",
    "#                         final_D = D\n",
    "#                         final_Q = Q\n",
    "#                         final_S = S\n",
    "\n",
    "#                 except:\n",
    "#                     pass\n",
    "\n",
    "# print(f'Our model that minimizes MSE on the training data is the SARIMA(4,0,6) x ({final_P},{final_D},{final_Q},{final_S})')\n",
    "# print(f'This model MSE = {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Matt Brem's grid search loop\n",
    "# Starting MSE and (P, D, Q).\n",
    "\n",
    "# mse = 99 * (10 ** 16)\n",
    "# final_P = 0\n",
    "# final_D = 0\n",
    "# final_Q = 0\n",
    "# final_S = 0\n",
    "\n",
    "# for P in range(2):\n",
    "#     for Q in range(2):\n",
    "#         for D in range(2):\n",
    "#             for S in range(0,25,12):\n",
    "#                 try:\n",
    "#                     # Instantiate SARIMA model.\n",
    "#                     sarima = SARIMAX(endog = y_train_hasp],\n",
    "#                                      order = (6, 0, 6),              # (p, d, q)\n",
    "#                                      seasonal_order = (P, D, Q, S),  # (P, D, Q, S)\n",
    "#                                      exog = X_train_sc_hasp) \n",
    "\n",
    "#                     # Fit SARIMA model.\n",
    "#                     model = sarima.fit()\n",
    "\n",
    "#                     # Generate predictions based on training set.\n",
    "#                     preds = model.predict()\n",
    "\n",
    "#                     # Evaluate predictions.\n",
    "#                     print(f'MSE for (6,0,6) x ({P},{D},{Q},{S}) ... {mean_squared_error(train[\"hasp_price_per_mwh\"], preds)}')\n",
    "\n",
    "#                     # Save for final report.\n",
    "#                     if mse > mean_squared_error(train['hasp_price_per_mwh'], preds):\n",
    "#                         mse = mean_squared_error(train['hasp_price_per_mwh'], preds)\n",
    "#                         final_P = P\n",
    "#                         final_D = D\n",
    "#                         final_Q = Q\n",
    "#                         final_S = S\n",
    "\n",
    "#                 except:\n",
    "#                     pass\n",
    "\n",
    "# print(f'Our model that minimizes MSE on the training data is the SARIMA(6,0,6) x ({final_P},{final_D},{final_Q},{final_S})')\n",
    "# print(f'This model MSE = {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and fit the models with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/anaconda3/envs/ds2/lib/python3.6/site-packages/statsmodels/tsa/base/tsa_model.py:225: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  ' ignored when e.g. forecasting.', ValueWarning)\n",
      "/Users/owner/anaconda3/envs/ds2/lib/python3.6/site-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for (4,0,6) x (0,1,0,24) ... 105.45\n"
     ]
    }
   ],
   "source": [
    "P = 0\n",
    "D = 1\n",
    "Q = 0\n",
    "S = 24\n",
    "\n",
    "# Instantiate SARIMA model.\n",
    "dam_sarimax01024 = SARIMAX(endog = y_train_dam,\n",
    "                 order = (4, 0, 6),              # (p, d, q)\n",
    "                 seasonal_order = (P, D, Q, S),  # (P, D, Q, S)\n",
    "                 exog = X_train_sc_dam) \n",
    "\n",
    "# Fit SARIMA model.\n",
    "dam_sarimax01024_model = dam_sarimax01024.fit()\n",
    "\n",
    "# Generate predictions based on training set.\n",
    "dam_sarimax01024_preds = dam_sarimax01024_model.predict()\n",
    "\n",
    "# Evaluate predictions.\n",
    "print(f'MSE for (4,0,6) x ({P},{D},{Q},{S}) ... {mean_squared_error(train[\"dam_price_per_mwh\"], dam_sarimax01024_preds):.2f}')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../fitted_models/dam_sarimax01024_model.pkl', 'wb') as f:\n",
    "    pickle.dump(dam_sarimax01024_model, f)\n",
    "    \n",
    "with open('../data/predictions/dam_sarimax01024_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(dam_sarimax01024_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/anaconda3/envs/ds2/lib/python3.6/site-packages/statsmodels/tsa/base/tsa_model.py:225: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  ' ignored when e.g. forecasting.', ValueWarning)\n",
      "/Users/owner/anaconda3/envs/ds2/lib/python3.6/site-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for (6,0,6) x (0,1,0,24) ... 1497.90\n"
     ]
    }
   ],
   "source": [
    "P = 0\n",
    "D = 1\n",
    "Q = 0\n",
    "S = 24\n",
    "\n",
    "# Instantiate SARIMA model.\n",
    "hasp_sarimax01024 = SARIMAX(endog = y_train_hasp,\n",
    "                 order = (6, 0, 6),              # (p, d, q)\n",
    "                 seasonal_order = (P, D, Q, S),  # (P, D, Q, S)\n",
    "                 exog = X_train_sc_hasp) \n",
    "\n",
    "# Fit SARIMA model.\n",
    "hasp_sarimax01024_model = hasp_sarimax01024.fit()\n",
    "\n",
    "# Generate predictions based on training set.\n",
    "hasp_sarimax01024_preds = hasp_sarimax01024_model.predict()\n",
    "\n",
    "# Evaluate predictions.\n",
    "print(f'MSE for (6,0,6) x ({P},{D},{Q},{S}) ... {mean_squared_error(train[\"hasp_price_per_mwh\"], hasp_sarimax01024_preds):.2f}')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../fitted_models/hasp_sarimax01024_model.pkl', 'wb') as f:\n",
    "    pickle.dump(hasp_sarimax01024_model, f)\n",
    "    \n",
    "with open('../data/predictions/hasp_sarimax01024_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(hasp_sarimax01024_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "295px",
    "width": "236px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226.641px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
